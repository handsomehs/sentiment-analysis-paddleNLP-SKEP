{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 【PaddleNLP】千言数据集：情感分析——SKEP\n",
    "本项目使用预训练模型[SKEP](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/skep)完成[千言数据集：情感分析](https://aistudio.baidu.com/aistudio/competition/detail/50)比赛  \n",
    "包含三种子任务，句子级情感分类、评价对象级情感分类、观点抽取  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 前言\n",
    "大家好  \n",
    "该比赛为[NLP打卡营](https://aistudio.baidu.com/aistudio/course/introduce/24177)的大作业。以下为通过本次课程所学内容，实现情感分析任务的代码。  \n",
    "<br>\n",
    "<br>\n",
    "先上结果：  \n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/f794a990ab54456e91d88141c6d6501b392ff162fa5f4e2089eca11b56974cf8\" width = \"900\" height = \"200\" alt=\"图片名称\" align=center />\n",
    "<br>\n",
    "<br>  \n",
    "**由于刚刚入门+没有任何机器学习的基础，所以结果不是很好，还有很大的调参和处理空间。  \n",
    "觉得有帮助的话，麻烦大家点个star和fork，谢谢啦！！！**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "强烈推荐大家参考以下源码：  \n",
    "- [SKEP example非常重要！！！](https://github.com/PaddlePaddle/PaddleNLP/tree/091ce4eb5a8ccd1d56c9caf1bd1a9aa3151f1d89/examples/sentiment_analysis/skep)\n",
    "- [load_dataset](https://github.com/PaddlePaddle/PaddleNLP/tree/06368bf7206ff9c6cfdbf34f9d79a656517a9302/paddlenlp/datasets)  \n",
    "- 可以给上面PaddleNLP的链接点个star哦！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "发现的好东西，但还没使用：[VisualDL2.2](https://aistudio.baidu.com/aistudio/projectdetail/1990920?channelType=0&channel=0)可以实现超参可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据集准备\n",
    "把所有数据集压缩包放在work文件夹下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据集到data文件夹, 注意每次打开都要执行，重启无需执行\r\n",
    "!unzip -q work/ChnSentiCorp.zip -d data\r\n",
    "!unzip -q work/NLPCC14-SC.zip -d data\r\n",
    "!unzip -q work/SE-ABSA16_CAME.zip -d data\r\n",
    "!unzip -q work/SE-ABSA16_PHNS.zip -d data\r\n",
    "!unzip -q work/COTE-BD.zip -d data\r\n",
    "!unzip -q work/COTE-DP.zip -d data\r\n",
    "!unzip -q work/COTE-MFW.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 更新paddlenlp\r\n",
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、 句子级情感分析\n",
    "对给定的一段文本进行情感极性分类，常用于影评分析、网络论坛舆情分析等场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据读入--句子级数据\n",
    "包含:\n",
    "- load_ds：可从训练集划分验证集\n",
    "- load_test：读入test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import random\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "\r\n",
    "# for train and dev sets\r\n",
    "def load_ds(datafiles, split_train=False, dev_size=0):\r\n",
    "    '''\r\n",
    "    intput:\r\n",
    "        datafiles -- str or list[str] -- the path of train or dev sets\r\n",
    "        split_train -- Boolean -- split from train or not\r\n",
    "        dev_size -- int -- split how much data from train \r\n",
    "\r\n",
    "    output:\r\n",
    "        MapDataset\r\n",
    "    '''\r\n",
    "\r\n",
    "    datas = []\r\n",
    "\r\n",
    "    def read(ds_file):\r\n",
    "        with open(ds_file, 'r', encoding='utf-8') as fp:\r\n",
    "            next(fp)  # Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                data = line[:-1].split('\\t')\r\n",
    "                if len(data)==2:\r\n",
    "                    yield ({'text':data[1], 'label':int(data[0])})\r\n",
    "                elif len(data)==3:\r\n",
    "                    yield ({'text':data[2], 'label':int(data[1])})\r\n",
    "    \r\n",
    "    def write_tsv(tsv, datas):\r\n",
    "        with open(tsv, mode='w', encoding='UTF-8') as f:\r\n",
    "            for line in datas:\r\n",
    "                f.write(line)\r\n",
    "    \r\n",
    "    # 从train切出一部分给dev\r\n",
    "    def spilt_train4dev(train_ds, dev_size):\r\n",
    "        with open(train_ds, 'r', encoding='UTF-8') as f:\r\n",
    "            for i, line in enumerate(f):\r\n",
    "                datas.append(line)\r\n",
    "        datas_tmp=datas[1:] # title line should not shuffle\r\n",
    "        random.shuffle(datas_tmp) \r\n",
    "        if 1-os.path.exists(os.path.dirname(train_ds)+'/tem'):\r\n",
    "            os.mkdir(os.path.dirname(train_ds)+'/tem')\r\n",
    "        # remember the title line\r\n",
    "        write_tsv(os.path.dirname(train_ds)+'/tem/train.tsv', datas[0:1]+datas_tmp[:-dev_size])\r\n",
    "        write_tsv(os.path.dirname(train_ds)+'/tem/dev.tsv', datas[0:1]+datas_tmp[-dev_size:])\r\n",
    "        \r\n",
    "\r\n",
    "    if split_train:\r\n",
    "        if 1-isinstance(datafiles, str):\r\n",
    "            print(\"If you want to split the train, make sure that \\'datafiles\\' is a train set path str.\")\r\n",
    "            return None\r\n",
    "        if dev_size == 0:\r\n",
    "            print(\"Please set size of dev set, as dev_size=...\")\r\n",
    "            return None\r\n",
    "        spilt_train4dev(datafiles, dev_size)\r\n",
    "        datafiles = [os.path.dirname(datafiles)+'/tem/train.tsv', os.path.dirname(datafiles)+'/tem/dev.tsv']\r\n",
    "    \r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def load_test(datafile):\r\n",
    "    '''\r\n",
    "    intput:\r\n",
    "        datafile -- str -- the path of test set \r\n",
    "\r\n",
    "    output:\r\n",
    "        MapDataset\r\n",
    "    '''\r\n",
    "    \r\n",
    "    def read(test_file):\r\n",
    "        with open(test_file, 'r', encoding='UTF-8') as f:\r\n",
    "            for i, line in enumerate(f):\r\n",
    "                if i==0:\r\n",
    "                    continue\r\n",
    "                data = line[:-1].split('\\t')\r\n",
    "                yield {'text':data[1], 'label':'', 'qid':data[0]}\r\n",
    "\r\n",
    "    return MapDataset(list(read(datafile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. ChnSentiCorp\n",
    "一次只能执行一个数据集！！！  \n",
    "32 256 2e-5 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可以直接使用官方的load_dataset更加方便\r\n",
    "# 本文将从train分出一部分做dev，为统一格式，故采用自己定义的load函数\r\n",
    "\r\n",
    "# from paddlenlp.datasets import load_dataset\r\n",
    "# train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, dev_ds= load_ds(datafiles=['./data/ChnSentiCorp/train.tsv', './data/ChnSentiCorp/dev.tsv'])\n",
    "print(train_ds[0])\n",
    "print(dev_ds[0])\n",
    "print(type(train_ds[0]))\n",
    "\n",
    "test_ds = load_test(datafile='./data/ChnSentiCorp/test.tsv')\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. NLPCC14-SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, dev_ds = load_ds(datafiles='./data/NLPCC14-SC/train.tsv', split_train=True, dev_size=1000)\r\n",
    "print(train_ds[0])\r\n",
    "print(dev_ds[0])\r\n",
    "test_ds = load_test(datafile='./data/NLPCC14-SC/test.tsv')\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## SKEP模型构建\n",
    "\n",
    "PaddleNLP已经实现了SKEP预训练模型，可以通过一行代码实现SKEP加载。\n",
    "\n",
    "句子级情感分析模型是SKEP fine-tune 文本分类常用模型`SkepForSequenceClassification`。其首先通过SKEP提取句子语义特征，之后将语义特征进行分类。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "# num_classes: 两类，0和1,表示消极和积极\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=2)\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`SkepForSequenceClassification`可用于句子级情感分析和目标级情感分析任务。其通过预训练模型SKEP获取输入文本的表示，之后将文本表示进行分类。\n",
    "\n",
    "* `pretrained_model_name_or_path`：模型名称。支持\"skep_ernie_1.0_large_ch\"，\"skep_ernie_2.0_large_en\"。\n",
    "\t- \"skep_ernie_1.0_large_ch\"：是SKEP模型在预训练ernie_1.0_large_ch基础之上在海量中文数据上继续预训练得到的中文预训练模型；\n",
    "    - \"skep_ernie_2.0_large_en\"：是SKEP模型在预训练ernie_2.0_large_en基础之上在海量英文数据上继续预训练得到的英文预训练模型；\n",
    "    \n",
    "* `num_classes`: 数据集分类类别数。\n",
    "\n",
    "\n",
    "关于SKEP模型实现详细信息参考：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/skep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "from utils import create_dataloader\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`int`, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, qid\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_seq_length = 256\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练和评估（把每个数据集的结果分开存！！！）\n",
    "\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。\n",
    "\n",
    "\n",
    "**推荐超参设置：**\n",
    "\n",
    "* `max_seq_length=256`\n",
    "* `batch_size=32`\n",
    "* `learning_rate=2e-5`\n",
    "* `epochs=10`\n",
    "\n",
    "实际运行时可以根据显存大小调整batch_size和max_seq_length大小。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from utils import evaluate\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "ckpt_dir = \"skep_sentence\"\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# optimizer = paddle.optimizer.AdamW(\n",
    "#     learning_rate=2e-5,\n",
    "#     parameters=model.parameters())\n",
    "\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=2e-6,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.01, # test weight_decay\n",
    "    apply_decay_param_fun=lambda x: x in decay_params # test weight_decay\n",
    "    )\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        if global_step % 100 == 0:\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            evaluate(model, criterion, metric, dev_data_loader) \n",
    "            model.save_pretrained(save_dir)\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测结果（注意更改模型的路径)\n",
    "\n",
    "\n",
    "使用训练得到的模型还可以对文本进行情感预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "batch_size=24\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_sentence2t_weight_2e-6/model_1800_83700/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "label_map = {0: '0', 1: '1'}\n",
    "results = []\n",
    "model.eval()\n",
    "for batch in test_data_loader:\n",
    "    input_ids, token_type_ids, qids = batch\n",
    "    logits = model(input_ids, token_type_ids)\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    labels = [label_map[i] for i in idx]\n",
    "    qids = qids.numpy().tolist()\n",
    "    results.extend(zip(qids, labels))\n",
    "\n",
    "res_dir = \"./results/2_weight_2e-6/1800_83700\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "\n",
    "with open(os.path.join(res_dir, \"NLPCC14-SC.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for qid, label in results:\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、 目标级情感分析\n",
    "\n",
    "在电商产品分析场景下，除了分析整体商品的情感极性外，还细化到以商品具体的“方面”为分析主体进行情感分析（aspect-level），如下、：\n",
    "\n",
    "* 这个薯片口味有点咸，太辣了，不过口感很脆。\n",
    "\n",
    "关于薯片的**口味方面**是一个负向评价（咸，太辣），然而对于**口感方面**却是一个正向评价（很脆）。\n",
    "\n",
    "* 我很喜欢夏威夷，就是这边的海鲜太贵了。\n",
    "\n",
    "关于**夏威夷**是一个正向评价（喜欢），然而对于**夏威夷的海鲜**却是一个负向评价（价格太贵）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据读入--目标级数据集\n",
    "与句子级方法类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "\r\n",
    "\r\n",
    "def load_ds(datafiles, split_train=False, dev_size=0):\r\n",
    "    datas = []\r\n",
    "    def read(ds_file):\r\n",
    "        with open(ds_file, 'r', encoding='utf-8') as fp:\r\n",
    "            # if fp.readline().split('\\t')[0] == 'label':\r\n",
    "            next(fp)  # Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                data = line[:-1].split('\\t')\r\n",
    "                yield ({'text':data[1], 'text_pair':data[2], 'label':int(data[0])})\r\n",
    "    \r\n",
    "    def write_tsv(tsv, datas):\r\n",
    "        with open(tsv, mode='w', encoding='UTF-8') as f:\r\n",
    "            for line in datas:\r\n",
    "                f.write(line)\r\n",
    "\r\n",
    "    def spilt_train4dev(train_ds, dev_size):\r\n",
    "        with open(train_ds, 'r', encoding='UTF-8') as f:\r\n",
    "            for i, line in enumerate(f):\r\n",
    "                datas.append(line)\r\n",
    "        datas_tmp=datas[1:] # title line should not shuffle\r\n",
    "        random.shuffle(datas_tmp) \r\n",
    "        if 1-os.path.exists(os.path.dirname(train_ds)+'/tem'):\r\n",
    "            os.mkdir(os.path.dirname(train_ds)+'/tem')\r\n",
    "        # remember the title line\r\n",
    "        write_tsv(os.path.dirname(train_ds)+'/tem/train.tsv', datas[0:1]+datas_tmp[:-dev_size])\r\n",
    "        write_tsv(os.path.dirname(train_ds)+'/tem/dev.tsv', datas[0:1]+datas_tmp[-dev_size:])\r\n",
    "        \r\n",
    "\r\n",
    "    if split_train:\r\n",
    "        if 1-isinstance(datafiles, str):\r\n",
    "            print(\"If you want to split the train, make sure that \\'datafiles\\' is a train set path str.\")\r\n",
    "            return None\r\n",
    "        if dev_size == 0:\r\n",
    "            print(\"Please set size of dev set, as dev_size=...\")\r\n",
    "            return None\r\n",
    "        spilt_train4dev(datafiles, dev_size)\r\n",
    "        datafiles = [os.path.dirname(datafiles)+'/tem/train.tsv', os.path.dirname(datafiles)+'/tem/dev.tsv']\r\n",
    "    \r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\r\n",
    "\r\n",
    "def load_test(datafile):\r\n",
    "    \r\n",
    "    def read(test_file):\r\n",
    "        with open(test_file, 'r', encoding='UTF-8') as f:\r\n",
    "            for i, line in enumerate(f):\r\n",
    "                if i==0:\r\n",
    "                    continue\r\n",
    "                data = line[:-1].split('\\t')\r\n",
    "                yield {'text':data[1], 'text_pair':data[2]}\r\n",
    "\r\n",
    "    return MapDataset(list(read(datafile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. SE-ABSA16_PHNS\n",
    "一次只能执行一个数据集！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, dev_ds = load_ds(datafiles='./data/SE-ABSA16_PHNS/train.tsv', split_train=True, dev_size=100)\r\n",
    "print(train_ds[0])\r\n",
    "print(dev_ds[0])\r\n",
    "\r\n",
    "test_ds = load_test(datafile='./data/SE-ABSA16_PHNS/test.tsv')\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. SE-ABSA16_CAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, dev_ds = load_ds(datafiles='./data/SE-ABSA16_CAME/train.tsv', split_train=True, dev_size=100)\r\n",
    "print(train_ds[0])\r\n",
    "print(dev_ds[0])\r\n",
    "\r\n",
    "test_ds = load_test(datafile='./data/SE-ABSA16_CAME/test.tsv')\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SKEP模型构建\n",
    "\n",
    "目标级情感分析模型同样使用`SkepForSequenceClassification`模型，但目标级情感分析模型的输入不单单是一个句子，而是句对。一个句子描述“评价对象方面（aspect）”，另一个句子描述\"对该方面的评论\"。如下图所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "# num_classes: 两类，0和1,表示消极和积极\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "from utils import create_dataloader\n",
    "\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False,\n",
    "                    dataset_name=\"chnsenticorp\"):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "    \n",
    "    note: There is no need token type ids for skep_roberta_large_ch model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "        dataset_name((obj:`str`, defaults to \"chnsenticorp\"): The dataset name, \"chnsenticorp\" or \"sst-2\".\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"],\n",
    "        text_pair=example[\"text_pair\"],\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "\n",
    "max_seq_length=512\n",
    "batch_size=16\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练和评估\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import evaluate\n",
    "\n",
    "\n",
    "epochs = 12\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# optimizer = paddle.optimizer.AdamW(\n",
    "#     learning_rate=2e-6,\n",
    "#     parameters=model.parameters())\n",
    "\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=1e-6,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.01, # test weight_decay\n",
    "    apply_decay_param_fun=lambda x: x in decay_params # test weight_decay\n",
    "    )\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "ckpt_dir = \"skep_aspect\"\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        if global_step % 50 == 0:\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            #这里的evaluate直接复制的上面项目的，可能不对\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\n",
    "            model.save_pretrained(save_dir)\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测结果\n",
    "\n",
    "使用训练得到的模型还可以对评价对象进行情感预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def predict(model, data_loader, label_map):\n",
    "    \"\"\"\n",
    "    Given a prediction dataset, it gives the prediction results.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n",
    "\n",
    "label_map = {0: '0', 1: '1'}\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_aspect4_weight_1e-6/model_500_73000/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = predict(model, test_data_loader, label_map)\n",
    "\n",
    "res_dir = \"./results/4_weight_1e-6/500_73000\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "\n",
    "with open(os.path.join(res_dir, \"SE-ABSA16_CAME.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        f.write(str(idx)+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、 评价对象提取 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据读入--对象数据集\n",
    "对象 = [ ]   \n",
    ":("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import random\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "\r\n",
    "\r\n",
    "def load_ds(datafiles, split_train=False, dev_size=0):\r\n",
    "    datas = []\r\n",
    "\r\n",
    "    def read(ds_file):\r\n",
    "        with open(ds_file, 'r', encoding='utf-8') as fp:\r\n",
    "            # if fp.readline().split('\\t')[0] == 'label':\r\n",
    "            next(fp)  # Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                # print('1\\n')\r\n",
    "                line_stripped = line.strip().split('\\t')\r\n",
    "                if not line_stripped:\r\n",
    "                    continue\r\n",
    "\r\n",
    "                # dataset 中有 entity和text不在一行的。。。。。\r\n",
    "                try:\r\n",
    "                    example = [line_stripped[indice] for indice in (0,1)]\r\n",
    "                    entity, text = example[0], example[1]\r\n",
    "                    start_idx = text.index(entity)\r\n",
    "                except:\r\n",
    "                    # drop the dirty data\r\n",
    "                    continue\r\n",
    "\r\n",
    "                labels = [2] * len(text)\r\n",
    "                labels[start_idx] = 0\r\n",
    "                for idx in range(start_idx + 1, start_idx + len(entity)):\r\n",
    "                        labels[idx] = 1 \r\n",
    "                yield {\r\n",
    "                        \"tokens\": list(text),\r\n",
    "                        \"labels\": labels,\r\n",
    "                        \"entity\": entity\r\n",
    "                    }\r\n",
    "    \r\n",
    "    def write_tsv(tsv, datas):\r\n",
    "        with open(tsv, mode='w', encoding='UTF-8') as f:\r\n",
    "            for line in datas:\r\n",
    "                f.write(line)\r\n",
    "\r\n",
    "\r\n",
    "    def spilt_train4dev(train_ds, dev_size):\r\n",
    "        with open(train_ds, 'r', encoding='UTF-8') as f:\r\n",
    "            for i, line in enumerate(f):\r\n",
    "                datas.append(line)\r\n",
    "        datas_tmp=datas[1:] # title line should not shuffle\r\n",
    "        random.shuffle(datas_tmp) \r\n",
    "        if 1-os.path.exists(os.path.dirname(train_ds)+'/tem'):\r\n",
    "            os.mkdir(os.path.dirname(train_ds)+'/tem')\r\n",
    "        # remember the title line\r\n",
    "        write_tsv(os.path.dirname(train_ds)+'/tem/train.tsv', datas[0:1]+datas_tmp[:-dev_size])\r\n",
    "        write_tsv(os.path.dirname(train_ds)+'/tem/dev.tsv', datas[0:1]+datas_tmp[-dev_size:])\r\n",
    "        \r\n",
    "        \r\n",
    "    if split_train:\r\n",
    "        if 1-isinstance(datafiles, str):\r\n",
    "            print(\"If you want to split the train, make sure that \\'datafiles\\' is a train set path str.\")\r\n",
    "            return None\r\n",
    "        if dev_size == 0:\r\n",
    "            print(\"Please set size of dev set, as dev_size=...\")\r\n",
    "            return None\r\n",
    "        spilt_train4dev(datafiles, dev_size)\r\n",
    "        datafiles = [os.path.dirname(datafiles)+'/tem/train.tsv', os.path.dirname(datafiles)+'/tem/dev.tsv']\r\n",
    "    \r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\r\n",
    "\r\n",
    "\r\n",
    "def load_test(datafile):\r\n",
    "    \r\n",
    "    def read(test_file):\r\n",
    "        with open(test_file, 'r', encoding='UTF-8') as fp:\r\n",
    "             # if fp.readline().split('\\t')[0] == 'label':\r\n",
    "            next(fp)  # Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                # print('1\\n')\r\n",
    "                line_stripped = line.strip().split('\\t')\r\n",
    "                if not line_stripped:\r\n",
    "                    continue\r\n",
    "                # example = [line_stripped[indice] for indice in field_indices]\r\n",
    "                # example = [line_stripped[indice] for indice in (0,1)]\r\n",
    "\r\n",
    "                # dataset 中有 entity和text不在一行的。。。。。\r\n",
    "                try:\r\n",
    "                    example = [line_stripped[indice] for indice in (0,1)]\r\n",
    "                    entity, text = example[0], example[1]\r\n",
    "                except:\r\n",
    "                    # drop the dirty data\r\n",
    "                    continue\r\n",
    "\r\n",
    "                yield {\"tokens\": list(text)}\r\n",
    "\r\n",
    "    return MapDataset(list(read(datafile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. COTE-DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = load_ds(datafiles='./data/COTE-DP/train.tsv')\r\n",
    "print(train_ds[0])\r\n",
    "test_ds = load_test(datafile='./data/COTE-DP/test.tsv')\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6. COTE-BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = load_ds(datafiles='./data/COTE-BD/train.tsv')\r\n",
    "print(train_ds[0])\r\n",
    "test_ds = load_test(datafile='./data/COTE-BD/test.tsv')\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 7. COTE-MFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = load_ds(datafiles='./data/COTE-MFW/train.tsv')\r\n",
    "print(train_ds[1])\r\n",
    "test_ds = load_test(datafile='./data/COTE-MFW/test.tsv')\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SKEP模型构建\n",
    "和上面的有些不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import SkepCrfForTokenClassification, SkepTokenizer, SkepModel\r\n",
    "# num_classes: 三类，B/I/O\r\n",
    "skep = SkepModel.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "model = SkepCrfForTokenClassification(skep, num_classes=3)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # The COTE_DP dataset labels with \"BIO\" schema.\r\n",
    "# label_map = {label: idx for idx, label in enumerate(train_ds.label_list)}\r\n",
    "# # `no_entity_label` represents that the token isn't an entity.\r\n",
    "# # print(type(no_entity_label_idx))\r\n",
    "no_entity_label_idx = 2\r\n",
    "\r\n",
    "from functools import partial\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader\r\n",
    "\r\n",
    "\r\n",
    "def convert_example_to_feature(example,\r\n",
    "                               tokenizer,\r\n",
    "                               max_seq_len=512,\r\n",
    "                               no_entity_label=\"O\",\r\n",
    "                               is_test=False):\r\n",
    "    \"\"\"\r\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \r\n",
    "    to be used in a sequence-pair classification task.\r\n",
    "        \r\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\r\n",
    "    ::\r\n",
    "        - single sequence: ``[CLS] X [SEP]``\r\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\r\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\r\n",
    "    ::\r\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\r\n",
    "        | first sequence    | second sequence |\r\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\r\n",
    "    Args:\r\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\r\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \r\n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\r\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization.\r\n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\r\n",
    "        no_entity_label(obj:`str`, defaults to \"O\"): The label represents that the token isn't an entity. \r\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\r\n",
    "    Returns:\r\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\r\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\r\n",
    "        label(obj:`list[int]`, optional): The input label if not test data.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    tokens = example['tokens']\r\n",
    "    labels = example['labels']\r\n",
    "    tokenized_input = tokenizer(\r\n",
    "        tokens,\r\n",
    "        return_length=True,\r\n",
    "        is_split_into_words=True,\r\n",
    "        max_seq_len=max_seq_len)\r\n",
    "\r\n",
    "    input_ids = tokenized_input['input_ids']\r\n",
    "    token_type_ids = tokenized_input['token_type_ids']\r\n",
    "    seq_len = tokenized_input['seq_len']\r\n",
    "\r\n",
    "    if is_test:\r\n",
    "        return input_ids, token_type_ids, seq_len\r\n",
    "    else:\r\n",
    "        labels = labels[:(max_seq_len - 2)]\r\n",
    "        encoded_label = np.array([no_entity_label] + labels + [no_entity_label], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, seq_len, encoded_label\r\n",
    "\r\n",
    "\r\n",
    "max_seq_length=256\r\n",
    "batch_size=40\r\n",
    "trans_func = partial(\r\n",
    "    convert_example_to_feature,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_len=max_seq_length,\r\n",
    "    no_entity_label=no_entity_label_idx,\r\n",
    "    is_test=False)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token]),  # input ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token]),  # token type ids\r\n",
    "    Stack(dtype='int64'),  # sequence lens\r\n",
    "    Pad(axis=0, pad_val=no_entity_label_idx)  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "# dev_data_loader = create_dataloader(\r\n",
    "#     dev_ds,\r\n",
    "#     mode='dev',\r\n",
    "#     batch_size=batch_size,\r\n",
    "#     batchify_fn=batchify_fn,\r\n",
    "#     trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "from utils import evaluate\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# # test weight_decay\r\n",
    "# decay_params = [\r\n",
    "#         p.name for n, p in model.named_parameters()\r\n",
    "#         if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "#     ]\r\n",
    "# optimizer = paddle.optimizer.AdamW(\r\n",
    "#     learning_rate=1e-5,\r\n",
    "#     parameters=model.parameters(),\r\n",
    "#     weight_decay=0.01, # test weight_decay\r\n",
    "#     apply_decay_param_fun=lambda x: x in decay_params # test weight_decay\r\n",
    "#     )\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=1e-6,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "# metric = ChunkEvaluator(label_list=train_ds.label_list, suffix=True)\r\n",
    "metric = ChunkEvaluator(label_list=['B', 'I', 'O'], suffix=True)\r\n",
    "\r\n",
    "ckpt_dir = \"skep_opinion7_1e-6\"\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, seq_lens, labels = batch\r\n",
    "        loss = model(\r\n",
    "                input_ids, token_type_ids, seq_lens=seq_lens, labels=labels)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\r\n",
    "                    % (global_step, epoch, step, avg_loss,\r\n",
    "                       10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        if global_step % 200 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "            \r\n",
    "            file_name = os.path.join(save_dir, \"model_state.pdparam\")\r\n",
    "            # Need better way to get inner model of DataParallel\r\n",
    "            paddle.save(model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "\r\n",
    "def convert_example_to_feature(example, tokenizer, max_seq_length=512, is_test=False):\r\n",
    "    \"\"\"\r\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \r\n",
    "    to be used in a sequence-pair classification task.\r\n",
    "        \r\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\r\n",
    "    ::\r\n",
    "        - single sequence: ``[CLS] X [SEP]``\r\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\r\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\r\n",
    "    ::\r\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\r\n",
    "        | first sequence    | second sequence |\r\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\r\n",
    "    Args:\r\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\r\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \r\n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\r\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \r\n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\r\n",
    "    Returns:\r\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\r\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask. \r\n",
    "    \"\"\"\r\n",
    "    tokens = example[\"tokens\"]\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        tokens,\r\n",
    "        return_length=True,\r\n",
    "        is_split_into_words=True,\r\n",
    "        max_seq_len=max_seq_length)\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "    seq_len = encoded_inputs[\"seq_len\"]\r\n",
    "\r\n",
    "    return input_ids, token_type_ids, seq_len\r\n",
    "\r\n",
    "\r\n",
    "def parse_predict_result(predictions, seq_lens, label_map):\r\n",
    "    \"\"\"\r\n",
    "    Parses the prediction results to the label tag.\r\n",
    "    \"\"\"\r\n",
    "    pred_tag = []\r\n",
    "    for idx, pred in enumerate(predictions):\r\n",
    "        seq_len = seq_lens[idx]\r\n",
    "        # drop the \"[CLS]\" and \"[SEP]\" token\r\n",
    "        tag = [label_map[i] for i in pred[1:seq_len - 1]]\r\n",
    "        pred_tag.append(tag)\r\n",
    "    return pred_tag\r\n",
    "\r\n",
    "@paddle.no_grad()\r\n",
    "def predict(model, data_loader, label_map):\r\n",
    "    \"\"\"\r\n",
    "    Given a prediction dataset, it gives the prediction results.\r\n",
    "    Args:\r\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\r\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\r\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\r\n",
    "    \"\"\"\r\n",
    "    model.eval()\r\n",
    "    results = []\r\n",
    "    for input_ids, token_type_ids, seq_lens in data_loader:\r\n",
    "        preds = model(input_ids, token_type_ids, seq_lens=seq_lens)\r\n",
    "        tags = parse_predict_result(preds.numpy(), seq_lens.numpy(), label_map)\r\n",
    "        results.extend(tags)\r\n",
    "    return results\r\n",
    "\r\n",
    "# The COTE_DP dataset labels with \"BIO\" schema.\r\n",
    "label_map = {0: \"B\", 1: \"I\", 2: \"O\"}\r\n",
    "# `no_entity_label` represents that the token isn't an entity. \r\n",
    "no_entity_label_idx = 2\r\n",
    "\r\n",
    "batch_size=96\r\n",
    "\r\n",
    "trans_func = partial(\r\n",
    "    convert_example_to_feature,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token]),  # input ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token]),  # token type ids\r\n",
    "    Stack(dtype='int64'),  # sequence lens\r\n",
    "): [data for data in fn(samples)]\r\n",
    "\r\n",
    "test_data_loader = create_dataloader(\r\n",
    "    test_ds,\r\n",
    "    mode='test',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "\r\n",
    "# 根据实际运行情况，更换加载的参数路径，！！！注意有没有加载成功！！！\r\n",
    "params_path = 'skep_opinion7_1e-6/model_10200_10/model_state.pdparam'\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)\r\n",
    "else:\r\n",
    "    print(\"MODEL LOAD FAILURE\")\r\n",
    "    exit\r\n",
    "\r\n",
    "results = predict(model, test_data_loader, label_map)\r\n",
    "\r\n",
    "# 处理符号\r\n",
    "punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~·！@#￥%……&*（）——+-=“：’；、。，？》《{}'\r\n",
    "\r\n",
    "res_dir = \"./results/7_1e-6/10200_10\"\r\n",
    "if not os.path.exists(res_dir):\r\n",
    "    os.makedirs(res_dir)\r\n",
    "\r\n",
    "with open(os.path.join(res_dir, \"COTE_MFW.tsv\"), 'w', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, example in enumerate(test_ds.data):\r\n",
    "        tag = []\r\n",
    "        # to find \"B...I...I\"\r\n",
    "        for idx1, letter in enumerate(results[idx]):\r\n",
    "            if letter == 'B':\r\n",
    "                i = idx1\r\n",
    "                try:\r\n",
    "                    while(results[idx][i+1]=='I'):\r\n",
    "                        i = i+1\r\n",
    "                except:\r\n",
    "                    pass\r\n",
    "                tag.append(re.sub(r\"[%s]+\" %punc, \"\", \"\".join(example['tokens'][idx1:i+1])))\r\n",
    "        if tag == []:\r\n",
    "            # 找不到实体要预测为无，不然比赛对比会异常\r\n",
    "            tag.append('无')\r\n",
    "        f.write(str(idx)+\"\\t\"+\"\\x01\".join(tag)+\"\\n\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 提交\n",
    "**将预测文件结果压缩至zip文件，提交**[千言比赛网站](https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 总结\n",
    "本项目为PaddleNLP打卡营的大作业，包含以下不足：  \n",
    "没用交叉检验，直接从训练集拿出一部分做验证集  \n",
    "没有对数据进行分析和处理  \n",
    "不会调参（没经验）\n",
    "  \n",
    "这个项目是本人第一个用Paddle完成的项目，也是第一个深度学习的项目。  \n",
    "通过实践学习到了很多，也发现还有很多要学习的。项目中有不足的地方，还望大家在评论区中指出。  \n",
    "也欢迎大家**点赞、fork、关注！**\n",
    "\n",
    "我在AI Studio上获得白银等级，点亮4个徽章，来互关呀~ https://aistudio.baidu.com/aistudio/personalcenter/thirdview/815060"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
